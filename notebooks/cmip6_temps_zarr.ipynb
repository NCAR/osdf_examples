{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "642d5bb1-f56f-44f2-983b-c85dabae97f7",
   "metadata": {},
   "source": [
    "# Access CMIP6 zarr data from AWS using the osdf protocol and compute GMSTA\n",
    "- This workflow is inspired by https://gallery.pangeo.io/repos/pangeo-gallery/cmip6/global_mean_surface_temp.html\n",
    "\n",
    "## Table of Contents\n",
    "- [Section 1: Introduction](#Section-1:-Introduction) \n",
    "- [Section 2: Select Dask Cluster](#Section-2:-Select-Dask-Cluster) \n",
    "- [Section 3: Data Loading](#Section-3:-Data-Loading) \n",
    "- [Section 4: Data Analysis](#Section-4:-Data-Analysis) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae82b24-e497-4916-b8e5-c21132607f72",
   "metadata": {},
   "source": [
    "## Section 1: Introduction\n",
    "- Load python packkages\n",
    "- Load catalog url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee9f844-c4a4-4327-94a1-260a1aa1d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import dask\n",
    "from dask.diagnostics import progress\n",
    "from tqdm.autonotebook import tqdm\n",
    "import intake\n",
    "import fsspec\n",
    "import seaborn as sns\n",
    "import re\n",
    "import aiohttp\n",
    "from dask_jobqueue import PBSCluster\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18afd4d0-1d32-4078-85a9-eb7d305cd96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec.implementations.http as fshttp\n",
    "from pelicanfs.core import OSDFFileSystem,PelicanMap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4325202-4ce3-4e1e-9fdf-11dbd228f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rda_scratch = '/gpfs/csfs1/collections/rda/scratch/harshah'\n",
    "rda_url     =  'https://data.rda.ucar.edu/'\n",
    "cat_url     = rda_url +  'd850001/catalogs/osdf/cmip6-aws/cmip6-osdf-zarr.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055d157-7e3b-41db-8d67-fe2e52daf385",
   "metadata": {},
   "source": [
    "## Section 2: Select Dask Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2071ec-1edc-4b0f-abf3-0ef6abbe7c76",
   "metadata": {},
   "source": [
    "#### Select the Dask cluster type\n",
    "The default will be LocalCluster as that can run on any system.\n",
    "\n",
    "If running on a HPC computer with a PBS Scheduler, set to True. Otherwise, set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a68af9-f9c2-4eed-9a12-d0d5e5dc1c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PBS_SCHEDULER = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c04f89-4484-46d5-96ea-3250488f4674",
   "metadata": {},
   "source": [
    "If running on Jupyter server with Dask Gateway configured, set to True. Otherwise, set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dc9399-cbe9-4847-aab1-02d246511907",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_DASK_GATEWAY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48d9879-9f31-485a-8bb4-60907a274d49",
   "metadata": {},
   "source": [
    "#### Python function for a PBS cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fbe212-e8d1-4dd2-9163-cbb9611dd4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PBS cluster object\n",
    "def get_pbs_cluster():\n",
    "    \"\"\" Create cluster through dask_jobqueue.   \n",
    "    \"\"\"\n",
    "    from dask_jobqueue import PBSCluster\n",
    "    cluster = PBSCluster(\n",
    "        job_name = 'dask-osdf-24',\n",
    "        cores = 1,\n",
    "        memory = '4GiB',\n",
    "        processes = 1,\n",
    "        local_directory = rda_scratch + '/dask/spill',\n",
    "        log_directory = rda_scratch + '/dask/logs/',\n",
    "        resource_spec = 'select=1:ncpus=1:mem=4GB',\n",
    "        queue = 'casper',\n",
    "        walltime = '3:00:00',\n",
    "        #interface = 'ib0'\n",
    "        interface = 'ext'\n",
    "    )\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c2b856-adc4-4d8a-b8ce-1c5d7407c43d",
   "metadata": {},
   "source": [
    "#### Python function for a Gateway Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b021dc5a-f2be-45f4-ae43-3589c87ba484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gateway_cluster():\n",
    "    \"\"\" Create cluster through dask_gateway\n",
    "    \"\"\"\n",
    "    from dask_gateway import Gateway\n",
    "\n",
    "    gateway = Gateway()\n",
    "    cluster = gateway.new_cluster()\n",
    "    cluster.adapt(minimum=2, maximum=4)\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b678447b-a43a-4fce-abab-bcaa8ff9ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_cluster():\n",
    "    \"\"\" Create cluster using the Jupyter server's resources\n",
    "    \"\"\"\n",
    "    from distributed import LocalCluster, performance_report\n",
    "    cluster = LocalCluster()    \n",
    "\n",
    "    cluster.scale(6)\n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f65a4e-059e-4043-bac3-38fdbe1f8ee6",
   "metadata": {},
   "source": [
    "#### Python logic for a Local Cluster\n",
    "This uses True/False boolean logic based on the variables set in the previous cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20253001-b812-433e-9c23-43b1e11e8d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain dask cluster in one of three ways\n",
    "if USE_PBS_SCHEDULER:\n",
    "    cluster = get_pbs_cluster()\n",
    "elif USE_DASK_GATEWAY:\n",
    "    cluster = get_gateway_cluster()\n",
    "else:\n",
    "    cluster = get_local_cluster()\n",
    "\n",
    "# Connect to cluster\n",
    "from distributed import Client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff8442-8385-4574-9466-4c8dcd87ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the cluster and display cluster dashboard URL\n",
    "cluster.scale(8)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34631dec-365d-4f0e-850f-d97e1c7d2e3a",
   "metadata": {},
   "source": [
    "## Section 3: Data Loading\n",
    "- Load catalog and select data subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcef760-8b9c-4068-ad42-77c36b5959e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = intake.open_esm_datastore(cat_url)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb806e6-b742-4dda-b0a8-cf0591c0392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[eid for eid in col.df['experiment_id'].unique() if 'ssp' in eid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b82caf2-d4b7-400e-849e-3ff746627cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is currently a significant amount of data for these runs\n",
    "expts = ['historical', 'ssp245', 'ssp370']\n",
    "\n",
    "query = dict(\n",
    "    experiment_id=expts,\n",
    "    table_id='Amon',\n",
    "    variable_id=['tas'],\n",
    "    member_id = 'r1i1p1f1',\n",
    "    #activity_id = 'CMIP',\n",
    ")\n",
    "\n",
    "col_subset = col.search(require_all_on=[\"source_id\"], **query)\n",
    "col_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bbd94d-804f-48ec-9144-e3d97530b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_subset.df.groupby(\"source_id\")[[\"experiment_id\", \"variable_id\", \"table_id\",\"activity_id\"]].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd3d850-239a-411e-8a16-b201222df416",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_subset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bb6d56-47d2-41aa-8dc5-b6fc50f2feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dsets_osdf  = col_subset.to_dataset_dict()\n",
    "# print(f\"\\nDataset dictionary keys:\\n {dsets_osdf.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b747dfe5-8ce4-428d-8836-371d50e9abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_all_bounds(ds):\n",
    "    drop_vars = [vname for vname in ds.coords\n",
    "                 if (('_bounds') in vname ) or ('_bnds') in vname]\n",
    "    return ds.drop(drop_vars)\n",
    "\n",
    "def open_dset(df):\n",
    "    assert len(df) == 1\n",
    "    mapper = fsspec.get_mapper(df.zstore.values[0])\n",
    "    path = df.zstore.values[0][7:]+\".zmetadata\"\n",
    "    ds = xr.open_zarr(mapper, consolidated=True)\n",
    "    a_data = mapper.fs.get_access_data()\n",
    "    responses = a_data.get_responses(path)\n",
    "    print(responses)\n",
    "    return drop_all_bounds(ds)\n",
    "\n",
    "def open_delayed(df):\n",
    "    return dask.delayed(open_dset)(df)\n",
    "\n",
    "from collections import defaultdict\n",
    "dsets = defaultdict(dict)\n",
    "\n",
    "for group, df in col_subset.df.groupby(by=['source_id', 'experiment_id']):\n",
    "    dsets[group[0]][group[1]] = open_delayed(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909202ae-a905-431e-82df-b1b564471424",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets_ = dask.compute(dict(dsets))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f80a5-6df5-428c-a7ef-acf3b596a239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate global means\n",
    "def get_lat_name(ds):\n",
    "    for lat_name in ['lat', 'latitude']:\n",
    "        if lat_name in ds.coords:\n",
    "            return lat_name\n",
    "    raise RuntimeError(\"Couldn't find a latitude coordinate\")\n",
    "\n",
    "def global_mean(ds):\n",
    "    lat = ds[get_lat_name(ds)]\n",
    "    weight = np.cos(np.deg2rad(lat))\n",
    "    weight /= weight.mean()\n",
    "    other_dims = set(ds.dims) - {'time'}\n",
    "    return (ds * weight).mean(other_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147beaca-9d0e-4bd8-8614-3a6a7bca3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_da = xr.DataArray(expts, dims='experiment_id', name='experiment_id',\n",
    "                       coords={'experiment_id': expts})\n",
    "\n",
    "dsets_aligned = {}\n",
    "\n",
    "for k, v in tqdm(dsets_.items()):\n",
    "    expt_dsets = v.values()\n",
    "    if any([d is None for d in expt_dsets]):\n",
    "        print(f\"Missing experiment for {k}\")\n",
    "        continue\n",
    "\n",
    "    for ds in expt_dsets:\n",
    "        ds.coords['year'] = ds.time.dt.year\n",
    "\n",
    "    # workaround for\n",
    "    # https://github.com/pydata/xarray/issues/2237#issuecomment-620961663\n",
    "    dsets_ann_mean = [v[expt].pipe(global_mean).swap_dims({'time': 'year'})\n",
    "                             .drop_vars('time').coarsen(year=12).mean()\n",
    "                      for expt in expts]\n",
    "\n",
    "    # align everything with the 4xCO2 experiment\n",
    "    dsets_aligned[k] = xr.concat(dsets_ann_mean, join='outer',dim=expt_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c849a2-ecf1-4fab-9208-ecadc7c82e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with progress.ProgressBar():\n",
    "    dsets_aligned_ = dask.compute(dsets_aligned)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90619917-6997-45ae-9572-ed525b15c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ids = list(dsets_aligned_.keys())\n",
    "source_da = xr.DataArray(source_ids, dims='source_id', name='source_id',\n",
    "                         coords={'source_id': source_ids})\n",
    "\n",
    "big_ds = xr.concat([ds.reset_coords(drop=True)\n",
    "                    for ds in dsets_aligned_.values()],\n",
    "                    dim=source_da)\n",
    "\n",
    "big_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c296261-4bb8-4f46-95b8-5c230eaea053",
   "metadata": {},
   "source": [
    "### Observational time series data for comparison with ensemble spread\n",
    "<!-- obsDataURL = \"https://www.esrl.noaa.gov/psd/thredds/dodsC/Datasets/cru/hadcrut4/air.mon.anom.median.nc\" -->\n",
    "- The observational data we will use is the HadCRUT5 dataset from the UK Met Office\n",
    "- The data has been downloaded to NCAR's Research Data Archive (RDA) from https://www.metoffice.gov.uk/hadobs/hadcrut5/\n",
    "- We will use an OSDF file system to access this copy from the RDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46b8f6-1f87-4460-967d-f433cee84f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "osdf_fs = OSDFFileSystem()\n",
    "print(osdf_fs)\n",
    "#\n",
    "obs_url    = '/ncar/rda/d850001/HadCRUT.5.0.2.0.analysis.summary_series.global.monthly.nc'\n",
    "#obs_ds = xr.open_dataset(obs_data)\n",
    "obs_ds = xr.open_dataset(osdf_fs.open(obs_url,mode='rb'),engine='h5netcdf').tas_mean\n",
    "obs_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5653375-98a6-4322-9673-aa57e79c0725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs_url    = 'osdf:///ncar/rda/d850001/HadCRUT.5.0.2.0.analysis.summary_series.global.monthly.zarr'\n",
    "# print(obs_url)\n",
    "# #\n",
    "# obs_ds = xr.open_zarr(obs_url).tas_mean\n",
    "# # obs_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9fc281-f29a-477f-9cc0-4414fd5aedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute annual mean temperatures anomalies\n",
    "obs_gmsta = obs_ds.resample(time='YS').mean(dim='time')\n",
    "obs_gmsta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b0729f-a509-4950-a1b8-fe0dce1f97cf",
   "metadata": {},
   "source": [
    "## Section 4: Data Analysis\n",
    "- Calculate Global Mean Surface Temperature Anomaly (GMSTA)\n",
    "- Grab some observations/ ERA5 reanalysis data\n",
    "- Convert xarray datasets to dataframes\n",
    "- Use Seaborn to plot GMSTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf4e2f2-8178-4383-9f85-3ec22085f04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = big_ds.to_dataframe().reset_index()\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e3413-0d30-4b97-a362-9d4739252282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute anomaly w.r.t 1960-1990 baseline\n",
    "# Define the baseline period\n",
    "baseline_df = df_all[(df_all[\"year\"] >= 1960) & (df_all[\"year\"] <= 1990)]\n",
    "\n",
    "# Compute the baseline mean\n",
    "baseline_mean = baseline_df[\"tas\"].mean()\n",
    "\n",
    "# Compute anomalies\n",
    "df_all[\"tas_anomaly\"] = df_all[\"tas\"] - baseline_mean\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b334d61-6cda-4676-bed5-b10aea769a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_df = obs_gmsta.to_dataframe(name='tas_anomaly').reset_index()\n",
    "# obs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa94c6-5d36-4769-a8a7-f120daffe6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'time' to 'year' (keeping only the year)\n",
    "obs_df['year'] = obs_df['time'].dt.year\n",
    "\n",
    "# Drop the original 'time' column since we extracted 'year'\n",
    "obs_df = obs_df[['year', 'tas_anomaly']]\n",
    "obs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9c84c0-a9b1-4531-8fb3-20982ad70866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main plot\n",
    "g = sns.relplot(data=df_all, x=\"year\", y=\"tas_anomaly\",\n",
    "                hue='experiment_id', kind=\"line\", errorbar=\"sd\", aspect=2, palette=\"Set2\")  # Adjust the color palette)\n",
    "\n",
    "# Get the current axis from the FacetGrid\n",
    "ax = g.ax\n",
    "\n",
    "# Overlay the observational data in red\n",
    "sns.lineplot(data=obs_df, x=\"year\", y=\"tas_anomaly\",color=\"red\", \n",
    "             linestyle=\"dashed\", linewidth=2,label=\"Observations\", ax=ax)\n",
    "\n",
    "# Adjust the legend to include observations\n",
    "ax.legend(title=\"Experiment ID + Observations\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cdcfd1-5f8a-49cf-a2c0-e84ba0eba64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398a5df-0d8d-4343-be09-9a176ccca101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.relplot(data=df_all,x=\"year\", y=\"tas_anomaly\", hue='experiment_id',kind=\"line\", errorbar=\"sd\", aspect=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
